---
author: Faith O. Oyedemi
bibliography: references/refs.bib
csl: references/csl/cjp.csl
date: 2025-11-25
tags:
- neural networks
- neuroscience
- foundation
title: Notable Names Linked to Neural Networks
---

In my study of machine learning models and their underlying ideas, I
have come across several individuals whose contributions shaped not only
the technical development of neural networks, but also the way we think
about learning, cognition, and adaptation in both biological and
artificial systems. This is my personal way of acknowledging them.

I am fully aware that this list cannot be exhaustive, nor is it intended
to be definitive. Rather, it reflects the intellectual trail I have
followed so far --- names that repeatedly surface in foundational
literature, historical accounts, and modern implementations. I will
continue to update this as my understanding increases and as I encounter
more contributors whose work has influenced the field.

- **Donald Olding Hebb** --- Laid the conceptual foundation for synaptic
  learning through the principle now known as Hebbian learning, shaping
  how we think about associative memory and adaptation.
- **Jeffrey Locke Elman** --- Introduced recurrent architectures that
  demonstrated how temporal structure and context could be learned in
  sequences.
- **Josef "Sepp" Hochreiter** --- Co-developed the Long Short-Term
  Memory (LSTM) architecture, addressing the vanishing gradient problem
  in recurrent networks.
- **John Hopfield** --- Proposed the Hopfield network, illuminating the
  relationship between neural computation and energy-based models.
- **Jürgen Schmidhuber** --- Contributed extensively to sequence
  learning, self-improving systems, and the theoretical understanding of
  deep learning.
- **Karl Lashley** --- Challenged localized views of memory, influencing
  distributed representation theories in neural systems.
- **Michael Irwin Jordan** --- Shaped modern statistical learning theory
  and connections between probabilistic modeling and machine learning.
- **Santiago Ramón y Cajal** --- Established the neuron doctrine,
  fundamentally redefining our understanding of the nervous system.
- **Shun'ichi Amari** --- Advanced information geometry and theoretical
  frameworks for neural computation.
- **Teuvo Kohonen** --- Developed self-organizing maps, revealing
  structure in high-dimensional data through competitive learning.
- **Torsten Wiesel** --- Alongside Hubel, uncovered organizational
  principles of the visual cortex that inspired convolutional
  architectures.
- **Yann LeCun** --- Pioneered convolutional neural networks and
  demonstrated their practical power.
- **Yoshua Bengio** --- Advanced deep learning through theoretical
  insights and practical algorithms for representation learning.

Each of these names represents not just a milestone, but a particular
way of thinking about learning, structure, and generalization. In
acknowledging them, I am also implicitly situating my own learning path
within a broader historical and conceptual lineage --- one that
continues to evolve with every new paper, model, and idea encountered.
